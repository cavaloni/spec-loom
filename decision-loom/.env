DATABASE_URL="file:./prisma/dev.db"

OPENROUTER_API_KEY="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"
OPENROUTER_BASE_URL="https://openrouter.ai/api/v1"
OPENROUTER_MODEL_SUGGEST="google/gemini-3-flash-preview"
OPENROUTER_MODEL_SUMMARY="anthropic/claude-haiku-4.5"
OPENROUTER_MODEL_GENERATE="openai/gpt-5.2"
OPENROUTER_MODEL_REFLECT="google/gemini-3-pro-preview"

# Rate limiting disabled for local development
# UPSTASH_REDIS_REST_URL=
# UPSTASH_REDIS_REST_TOKEN=

# =============================================================================
# OBSERVABILITY / TELEMETRY
# =============================================================================

# Structured logging level (debug, info, warn, error)
LOG_LEVEL=debug

# OpenTelemetry - Traces export to Grafana Tempo, Jaeger, or any OTLP endpoint
# Leave empty to disable tracing
# Just these 2 lines in your .env
OTEL_EXPORTER_OTLP_ENDPOINT=https://otlp-gateway-prod-us-west-0.grafana.net/otlp
OTEL_SERVICE_NAME=decision-loom

# Langfuse - LLM observability (prompt/response tracking, token usage, costs)
# Sign up at https://cloud.langfuse.com or self-host
LANGFUSE_SECRET_KEY="sk-lf-966b20e4-b7e3-46f3-8310-7188675032e9"
LANGFUSE_PUBLIC_KEY="pk-lf-92b734e7-65c0-4f11-a6e5-2ff959706f45"
LANGFUSE_BASE_URL="https://cloud.langfuse.com"

# LLM client settings
OPENROUTER_TIMEOUT_MS=120000
OPENROUTER_MAX_RETRIES=0
